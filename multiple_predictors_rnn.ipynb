{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multiple_predictors_rnn",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahavir9008/HelloTest/blob/master/multiple_predictors_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOwkkbFiD_om"
      },
      "source": [
        "!pip install earthengine-api\n",
        "!earthengine authenticate\n",
        "!pip install geojson\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import ee\n",
        "import datetime as dt\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnCJ6i97hK6q",
        "outputId": "773be9b5-282a-44e2-a2a3-afc1e518394c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "print(gj)\n",
        "zwolle=ee.Geometry.Polygon([[[6.056213, 52.50849], [6.066513, 52.501176], [6.088142, 52.490881], [6.092863, 52.479958], [6.101875, 52.479748], [6.108913, 52.480376], [6.113205, 52.481264], [6.119299, 52.483042], [6.124363, 52.486544], [6.129684, 52.488216], [6.133118, 52.490411], [6.138182, 52.493076], [6.139297, 52.496369], [6.137238, 52.499451], [6.136379, 52.504624], [6.142559, 52.50943], [6.144791, 52.515281], [6.146507, 52.524368], [6.143932, 52.532305], [6.136379, 52.537108], [6.119986, 52.536169], [6.115351, 52.53904], [6.105738, 52.546244], [6.096125, 52.548644], [6.08737, 52.548331], [6.079216, 52.545461], [6.06883, 52.543999], [6.061277, 52.544521], [6.054583, 52.543999], [6.050291, 52.546296], [6.045141, 52.54734], [6.039648, 52.548331], [6.033125, 52.546557], [6.031065, 52.54259], [6.029091, 52.538048], [6.028833, 52.532462], [6.030636, 52.52912], [6.043253, 52.526405], [6.044111, 52.523533], [6.042137, 52.519407], [6.05072, 52.519355], [6.042824, 52.513087], [6.056213, 52.50849]]])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"features\": [{\"geometry\": {\"coordinates\": [[[6.056213, 52.50849], [6.066513, 52.501176], [6.088142, 52.490881], [6.092863, 52.479958], [6.101875, 52.479748], [6.108913, 52.480376], [6.113205, 52.481264], [6.119299, 52.483042], [6.124363, 52.486544], [6.129684, 52.488216], [6.133118, 52.490411], [6.138182, 52.493076], [6.139297, 52.496369], [6.137238, 52.499451], [6.136379, 52.504624], [6.142559, 52.50943], [6.144791, 52.515281], [6.146507, 52.524368], [6.143932, 52.532305], [6.136379, 52.537108], [6.119986, 52.536169], [6.115351, 52.53904], [6.105738, 52.546244], [6.096125, 52.548644], [6.08737, 52.548331], [6.079216, 52.545461], [6.06883, 52.543999], [6.061277, 52.544521], [6.054583, 52.543999], [6.050291, 52.546296], [6.045141, 52.54734], [6.039648, 52.548331], [6.033125, 52.546557], [6.031065, 52.54259], [6.029091, 52.538048], [6.028833, 52.532462], [6.030636, 52.52912], [6.043253, 52.526405], [6.044111, 52.523533], [6.042137, 52.519407], [6.05072, 52.519355], [6.042824, 52.513087], [6.056213, 52.50849]]], \"type\": \"Polygon\"}, \"properties\": {}, \"type\": \"Feature\"}], \"type\": \"FeatureCollection\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1huqS06cJul"
      },
      "source": [
        "def EarthEngineDownloader(bbox, EEImageName=\"COPERNICUS/S5P/OFFL/L3_NO2\", StartDate='2018-01-01', EndDate = '2020-10-12',  MyDescription=\"No2_raster_data\",OutPutFile= \"Raster\"):\n",
        "    #Initiate ee package\n",
        "    ee.Initialize()\n",
        "    \n",
        "    #Filtering and colelcting images from google Earth Engine\n",
        "    EarthEngineImageFilter = ee.ImageCollection(EEImageName).filterDate(StartDate,EndDate).filterBounds(bbox).first() \n",
        "    print(EarthEngineImageFilter.getInfo())\n",
        "    \n",
        "    #Downloading Google Earth data\n",
        "    task = ee.batch.Export.image.toDrive(\n",
        "      image= EarthEngineImageFilter,\n",
        "      description= MyDescription,\n",
        "      region = ee.Geometry.Polygon(bbox),\n",
        "      fileNamePrefix= OutPutFile,\n",
        "      fileFormat= 'GeoTIFF' )\n",
        "           \n",
        "    task.start()\n",
        "    print(task.status())\n",
        "    return task\n",
        "    \n",
        "zwolle=ee.Geometry.Polygon([[[6.056213, 52.50849], [6.066513, 52.501176], [6.088142, 52.490881], [6.092863, 52.479958], [6.101875, 52.479748], [6.108913, 52.480376], [6.113205, 52.481264], [6.119299, 52.483042], [6.124363, 52.486544], [6.129684, 52.488216], [6.133118, 52.490411], [6.138182, 52.493076], [6.139297, 52.496369], [6.137238, 52.499451], [6.136379, 52.504624], [6.142559, 52.50943], [6.144791, 52.515281], [6.146507, 52.524368], [6.143932, 52.532305], [6.136379, 52.537108], [6.119986, 52.536169], [6.115351, 52.53904], [6.105738, 52.546244], [6.096125, 52.548644], [6.08737, 52.548331], [6.079216, 52.545461], [6.06883, 52.543999], [6.061277, 52.544521], [6.054583, 52.543999], [6.050291, 52.546296], [6.045141, 52.54734], [6.039648, 52.548331], [6.033125, 52.546557], [6.031065, 52.54259], [6.029091, 52.538048], [6.028833, 52.532462], [6.030636, 52.52912], [6.043253, 52.526405], [6.044111, 52.523533], [6.042137, 52.519407], [6.05072, 52.519355], [6.042824, 52.513087], [6.056213, 52.50849]]])\n",
        "EarthEngineDownloader(bbox=zwolle)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3qTo2a_aUF5"
      },
      "source": [
        "# Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EztE3hbsXhyO"
      },
      "source": [
        "#Cleaning data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbZxOJxPoTGA",
        "outputId": "825d4527-1083-4b62-abb2-6e54c2f0c97e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os \n",
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUct5Q3TaY-J"
      },
      "source": [
        "# Importing Training Set\n",
        "raw_dataset_rainfall= pd.read_csv('Rainfall_zwolle.csv')\n",
        "\n",
        "raw_dataset_no2 = pd.read_csv('no2.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3SGGnMiSSdX"
      },
      "source": [
        "##remove unneccassary columns in rainfall dataframe\n",
        "col_to_remove_rainfall = ('X','Y', 'Height', 'StationName', 'ReportEndDateTime', 'ExternalSiteId', 'RainfallRate_MillimetrePerHour', 'OBJECTID' )\n",
        "\n",
        "for name in col_to_remove_rainfall:\n",
        "  if name in raw_dataset_rainfall.columns:\n",
        "    raw_dataset_rainfall = raw_dataset_rainfall.drop(name,1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#creating a dates column in the rainfall dataframe that matches the dates format in the NO2 dataframe\n",
        "dates=[]\n",
        "for i in range(len(raw_dataset_rainfall)):\n",
        "  dates.append(raw_dataset_rainfall['ReportStartDateTime'][i])\n",
        "\n",
        "cleaned_dates_rainfall=[]\n",
        "for date in dates:\n",
        "  value = date.rsplit(' ', 1)[0]\n",
        "  value = value.replace('/','-')\n",
        "  value = dt.datetime.strptime(value, '%Y-%m-%d')\n",
        "  cleaned_dates_rainfall.append(value)\n",
        "dates=[] #emptying list\n",
        "\n",
        "raw_dataset_rainfall['dates'] = cleaned_dates_rainfall\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz2hNTMcULKE",
        "outputId": "579656a6-be6b-4538-a947-f048fcbab8d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "##remove unneccassary columns in rainfall dataframe\n",
        "col_to_remove_no2 =('wkt_geom', 'id', 'timestam_1', 'unit_NO2', 'unit_PM10', 'unit_RH', 'unit_P', 'value_PM10', 'value_RH', 'value_P')\n",
        "for name in col_to_remove_no2:\n",
        "  if name in raw_dataset_no2.columns:\n",
        "    raw_dataset_no2 = raw_dataset_no2.drop(name,1)\n",
        "\n",
        "dates=[]\n",
        "for i in range(len(raw_dataset_no2)):\n",
        "  dates.append(raw_dataset_no2['timestamp_'][i])\n",
        "\n",
        "cleaned_dates_no2=[]\n",
        "for date in dates:\n",
        "  value = dt.datetime.strptime(date, '%Y-%m-%d')\n",
        "  cleaned_dates_no2.append(value)\n",
        "dates=[] #emptying list\n",
        "\n",
        "raw_dataset_rainfall['dates'] = cleaned_dates_no2\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8e79d025b375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#emptying list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mraw_dataset_rainfall\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dates'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_dates_no2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3040\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3114\u001b[0m         \"\"\"\n\u001b[1;32m   3115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, key, value, broadcast)\u001b[0m\n\u001b[1;32m   3761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m             \u001b[0;31m# turn me into an ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3763\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3764\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3765\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_index\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         raise ValueError(\n\u001b[0;32m--> 748\u001b[0;31m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0;34m\"does not match length of index \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (122552) does not match length of index (538937)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXUfENT9ZlOD"
      },
      "source": [
        "##creating list of dataframes for each rainfall sensor\n",
        "rainfall_sensors = raw_dataset_rainfall.SiteID.unique()\n",
        "dataset_per_r_sensor =[] #list of dataframes for each rainfall sensor\n",
        "\n",
        "count=0\n",
        "while count <=len(rainfall_sensors):\n",
        "  for sensor in rainfall_sensors:\n",
        "    dataset_per_r_sensor.append(raw_dataset_rainfall.loc[raw_dataset_rainfall['SiteID'] == sensor])\n",
        "    dataset_per_r_sensor[count].sort_values('ReportStartDateTime' , ascending=True)\n",
        "    count+=1\n",
        "\n",
        "##remove rows of first date \n",
        "  ##in order to start from 00:00\n",
        "  ##should be 144 records per day\n",
        "for dataset in dataset_per_r_sensor:\n",
        "  min_date = dataset.ReportStartDateTime.min()\n",
        "  dataset=dataset[dataset.ReportStartDateTime != min_date]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PylXvzRrZm9h"
      },
      "source": [
        "##creating list of dataframes for each no2 sensor\n",
        "no2_sensors = raw_dataset_no2.label.unique()\n",
        "dataset_per_no2_sensor =[] #list of dataframes for each NO2 sensor\n",
        "\n",
        "count=0\n",
        "while count <=len(no2_sensors):\n",
        "  for sensor in no2_sensors:\n",
        "    dataset_per_no2_sensor.append(raw_dataset_no2.loc[raw_dataset_no2['label'] == sensor])\n",
        "    dataset_per_no2_sensor[count].sort_values('timestamp_' , ascending=True)\n",
        "    count+=1\n",
        "\n",
        "##remove rows for first date\n",
        "  ##in order to start from 00:00\n",
        "  ##should be 24 records per date\n",
        "for dataset in dataset_per_no2_sensor:\n",
        "  min_date = dataset.timestamp_.min()\n",
        "  dataset=dataset[dataset.timestamp_ != min_date]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88JUKArYg-tq",
        "outputId": "622ef38a-a5dd-4e30-c7f6-cc25cfd27d4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "##selecting rainfall and no2 sensors that are in close proximity and combining their datasets\n",
        "distances_dataset = pd.read_csv('distances.csv')\n",
        "rainfall_no2_sensor_pair = []\n",
        "\n",
        "for i in range(len(distances_dataset)):\n",
        "  if distances_dataset.iloc[i,4]<=300:\n",
        "    dist=(distances_dataset.iloc[i,1], distances_dataset.iloc[i,3])\n",
        "    rainfall_no2_sensor_pair.append(dist)\n",
        "\n",
        "#print(rainfall_no2_sensor_pair)\n",
        "\n",
        "\n",
        "combined_dataset=[]\n",
        "for pair in rainfall_no2_sensor_pair:\n",
        "  for dataset in dataset_per_no2_sensor:\n",
        "    for dataframe in dataset_per_r_sensor:\n",
        "      if pair[0]== dataset.iloc[0,1] and pair[1]==dataframe.iloc[0,0]: #check if pair exists in rainfall_no2_sensor_pair\n",
        "        no2_min = dataset.timestamp_.min\n",
        "        no2_max = dataset.timestamp_.max\n",
        "        dataframe = dataframe['dates']>= no2_max and dataframe['dates']<=no2_min\n",
        "\n",
        "\n",
        "        #clip all values outside of min and max for rainfall data\n",
        "        # check number of values for single date in rainfall data and for single date in no2 data\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##make sure dates are matching\n",
        "##remove dates that are not present in no2 data\n",
        "##if not genrate no2 data to match number of rainfall data records"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-b8f6f8ef3936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mno2_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mno2_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>=\u001b[0m \u001b[0mno2_max\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m<=\u001b[0m\u001b[0mno2_min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/ops.pyx\u001b[0m in \u001b[0;36mpandas._libs.ops.scalar_compare\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'str' and 'method'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkOlVbD4ccLj"
      },
      "source": [
        "\n",
        "\n",
        "##splitting each remaining dataset into train/test\n",
        "  ##train_x n test_x == rainfall dataset (train proportion)\n",
        "  ##train_y and test_y == no2 dataset (test proportion)\n",
        "dataset_train=[]\n",
        "dataset_test=[]\n",
        "\n",
        "\n",
        "dataset_train_x = [] #list of dataframes for each sensor for training\n",
        "dataset_train_y=[]\n",
        "\n",
        "dataset_test_x = [] #list of dataframes for each sensor for testing\n",
        "dataset_test_y = []\n",
        "\n",
        "##Splitting each rainfall dataset into train and test sets\n",
        "train_split_ratio = 0.8 #select train split ratio\n",
        "for dataset in dataset_per_r_sensor:\n",
        "  train_len = int(train_split_ratio*len(dataset))\n",
        "  test_len = len(dataset) - train_len\n",
        "\n",
        "  dataset_train_sensors.append(dataset[:train_len])\n",
        "  dataset_test_sensors.append(dataset[train_len:-1])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "    \n",
        "##Extract columns required for feature training and for predictions\n",
        "\n",
        "\n",
        "\n",
        "#cols = list(dataset_train)[1:5]\n",
        "#dataset_train_sensors = dataset_train_sensors[cols]\n",
        "#dataset_test_sensors = dataset_test_sensors[cols]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPoL_NjVbBzn"
      },
      "source": [
        "##average out nan values using a window/kernel\n",
        "#this might take a minute\n",
        "\n",
        "window_size = 20 #select a window size to calculate average values for cells with no data\n",
        "for dataset in dataset_train_sensors:\n",
        "\n",
        "  \n",
        "  for c in range(len(dataset.columns)):\n",
        "    for r in range(len(dataset)):\n",
        "      if dataset.iloc[r,c]== np.nan:\n",
        "\n",
        "        window_pos = range(window_size)\n",
        "        window_neg = [ -x for x in window_pos]\n",
        "        window = window_pos+ window_neg\n",
        "\n",
        "        mean_values=[]\n",
        "        for w in window:\n",
        "          if dataset.iloc[r+w,c] >=0:\n",
        "            mean_values.append(dataset.iloc[r+w,c])\n",
        "        mean = (sum(mean_values))/mean_values.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXDtUZ3GEara"
      },
      "source": [
        "##scale data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "sc = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba7YTJBjaaKN"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ9AVNh3GKS_"
      },
      "source": [
        "# Part 1 - Data Preprocessing\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        " \n",
        "\n",
        "#Preprocess data for training by removing all commas\n",
        " \n",
        "dataset_train = dataset_train[cols].astype(str)\n",
        "for i in cols:\n",
        "    for j in range(0,len(dataset_train)):\n",
        "        dataset_train[i][j] = dataset_train[i][j].replace(\",\",\"\")\n",
        " \n",
        "dataset_train = dataset_train.astype(float)\n",
        " \n",
        " \n",
        "training_set = dataset_train.as_matrix() # Using multiple predictors.\n",
        " \n",
        "# Feature Scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        " \n",
        "sc = StandardScaler()\n",
        "training_set_scaled = sc.fit_transform(training_set)\n",
        " \n",
        "sc_predict = StandardScaler()\n",
        " \n",
        "sc_predict.fit_transform(training_set[:,0:1])\n",
        " \n",
        "# Creating a data structure with 60 timesteps and 1 output\n",
        "X_train = []\n",
        "y_train = []\n",
        " \n",
        "n_future = 20  # Number of days you want to predict into the future\n",
        "n_past = 60  # Number of past days you want to use to predict the future\n",
        " \n",
        "for i in range(n_past, len(training_set_scaled) - n_future + 1):\n",
        "    X_train.append(training_set_scaled[i - n_past:i, 0:5])\n",
        "    y_train.append(training_set_scaled[i+n_future-1:i + n_future, 0])\n",
        " \n",
        "X_train, y_train = np.array(X_train), np.array(y_train)\n",
        " \n",
        "# Part 2 - Building the RNN\n",
        " \n",
        "# Import Libraries and packages from Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        " \n",
        "# Initializing the RNN\n",
        "regressor = Sequential()\n",
        " \n",
        "# Adding fist LSTM layer and Drop out Regularization\n",
        "regressor.add(LSTM(units=10, return_sequences=True, input_shape=(n_past, 4)))\n",
        " \n",
        " \n",
        "# Part 3 - Adding more layers\n",
        "# Adding 2nd layer with some drop out regularization\n",
        "regressor.add(LSTM(units=4, return_sequences=False))\n",
        " \n",
        "# Output layer\n",
        "regressor.add(Dense(units=1, activation='linear'))\n",
        " \n",
        "# Compiling the RNN\n",
        "regressor.compile(optimizer='adam', loss=\"mean_squared_error\")  # Can change loss to mean-squared-error if you require.\n",
        " \n",
        "# Fitting RNN to training set using Keras Callbacks. Read Keras callbacks docs for more info.\n",
        " \n",
        "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
        "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "tb = TensorBoard('logs')\n",
        " \n",
        "history = regressor.fit(X_train, y_train, shuffle=True, epochs=100,\n",
        "                        callbacks=[es, rlr,mcp, tb], validation_split=0.2, verbose=1, batch_size=64)\n",
        " \n",
        " \n",
        "# Predicting the future.\n",
        "#--------------------------------------------------------\n",
        "# The last date for our training set is 30-Dec-2016.\n",
        "# Lets now try predicting the stocks for the dates in the test set.\n",
        " \n",
        "# The dates on our test set are:\n",
        "# 3,4,5,6,9,10,11,12,13,17,18,19,20,23,24,25,26,27,30,31-Jan-2017\n",
        " \n",
        "# Now, the latest we can predict into our test set is to the 19th since the last date on training is 30-Dec-2016. \n",
        "# 20 days into the future from the latest day in our training set is 19-Dec-2016. Right?\n",
        "# Notice that we dont have some days in our test set, what we can do is to take the last 20 samples from the training set. \n",
        "# (Remember the last sample of our training set will predict the 19th of Jan 2017, the second last will predict the 18th, etc)\n",
        " \n",
        " \n",
        "# Lets first import the test_set.\n",
        "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
        "y_true = np.array(dataset_test['Open'])\n",
        "#Trim the test set to first 12 entries (till the 19th)\n",
        "y_true = y_true[0:12]\n",
        "predictions = regressor.predict(X_train[-20:])\n",
        " \n",
        " \n",
        "# We skip the 31-Dec, 1-Jan,2-Jan, etc to compare with the test_set\n",
        "predictions_to_compare = predictions[[3,4,5,6,9,10,11,12,13,17,18,19]]\n",
        "y_pred = sc_predict.inverse_transform(predictions_to_compare)\n",
        " \n",
        " \n",
        " \n",
        "hfm, = plt.plot(y_pred, 'r', label='predicted_stock_price')\n",
        "hfm2, = plt.plot(y_true,'b', label = 'actual_stock_price')\n",
        " \n",
        "plt.legend(handles=[hfm,hfm2])\n",
        "plt.title('Predictions and Actual Price')\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Stock Price Future')\n",
        "plt.savefig('graph.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()\n",
        " \n",
        " \n",
        " \n",
        "hfm, = plt.plot(sc_predict.inverse_transform(y_train), 'r', label='actual_training_stock_price')\n",
        "hfm2, = plt.plot(sc_predict.inverse_transform(regressor.predict(X_train)),'b', label = 'predicted_training_stock_price')\n",
        " \n",
        "plt.legend(handles=[hfm,hfm2])\n",
        "plt.title('Predictions vs Actual Price')\n",
        "plt.xlabel('Sample index')\n",
        "plt.ylabel('Stock Price Training')\n",
        "plt.savefig('graph_training.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "plt.close()\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}